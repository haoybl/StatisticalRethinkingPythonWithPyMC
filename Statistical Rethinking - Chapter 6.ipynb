{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame( \n",
    "    np.array([[438, 452, 612, 521, 752, 871, 1350], \n",
    "     [37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5]]).T,\n",
    "    columns=['brain', 'mass'])\n",
    "\n",
    "data['mass'] = (data['mass'] - data['mass'].mean()) / data['mass'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brain</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>438.0</td>\n",
       "      <td>-0.779467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>452.0</td>\n",
       "      <td>-0.917020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>612.0</td>\n",
       "      <td>-1.008722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>521.0</td>\n",
       "      <td>-0.366808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>752.0</td>\n",
       "      <td>0.917020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brain      mass\n",
       "0  438.0 -0.779467\n",
       "1  452.0 -0.917020\n",
       "2  612.0 -1.008722\n",
       "3  521.0 -0.366808\n",
       "4  752.0  0.917020"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Information and uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information: Reduction in uncertainty derived from learning an outcome.\n",
    "\n",
    "How much you stand to learn when you know a new outcome. Los Angeles always sunny, Seattle always rainy -- you don't learn much from observing a new outcome. Atlanta is very unpredictable -- do you learn more here when you observe a new outcome? Yes, in Atlanta you are no longer uncertain about the weather tomorrow, once tomorrow is here (you were uncertain before, because Atlanta is more unpredictable than the others, but once it arrives, you learn a lot).\n",
    "\n",
    "The task is to quantify (information=) how much uncertainty is reduced when you learn an outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to measure uncertainty. Some desiderata for an uncertainty measure:\n",
    "\n",
    "1. **The measure of uncertainty should be continuous**. If not, then an arbitrarily small change in any of the probabilities would result in a massive change in uncertainty.\n",
    "2. **The measure of uncertainty should increase as the number of possible events increase**. For example, suppose there are two cities that need weather forecasts. In the first city, it rains on half the days of the year. In the second city, it rains, snows or shines, each on 1 of 3 days in the year. We'd like our measure of uncertainty to be larger in the second city, where there is one more kind of event to predict.\n",
    "3. **The measure of uncertainty should be additive**. What this means is that if we measure the uncertainty about rain or shine, then the uncertainty about hot or cold, the uncertainty over the four combinations of these events should be the sum of the separate uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we use is called **information entropy**:\n",
    "\n",
    "$$ H(p) = - E log(p_i) = \\sum p_i log(p_i) $$\n",
    "\n",
    "Which translates to: \n",
    "_The uncertainty contained in a probability distribution is the average log-probability of an event._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6108643020548935"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([0.3, 0.7])\n",
    "-np.sum( p * np.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056001534354847345"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Less uncertainty \n",
    "\n",
    "p = np.array([0.01, 0.99])\n",
    "-np.sum( p * np.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81880845622287701"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More uncertainty when there are more possible events\n",
    "\n",
    "p = np.array([0.15, 0.15, 0.7])\n",
    "-np.sum( p * np.log(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 From entropy to accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divergence**: The additional uncertainty induced by using probabilities from one distribution to describe another distribution. (K-L divergence)\n",
    "\n",
    "$$D_{KL}(p, q) = \\sum p_i (log(p_i) - log(q_i)) = \\sum p_i log(p_i \\div q_i)$$\n",
    "\n",
    "$$D_{KL}(p, p) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model is highly uncertain, the distance to another model is not that great, because you didn't know much about the world in the first place (travelling from Earth to Mars). If your model is very confident (Mars 0.99 is land), then the distance to some other distribution of land is higher because \"the surprise\" potential. If you learn the distribution of land from Mars, you'll be very surprised by anything else.\n",
    "\n",
    "If your model is uncertain about how things happen, it won't have a big divergence from other distributions. The more certain your model is, the potential for it to more and more wrong = greater and greater divergence from the truth.\n",
    "\n",
    "Sometimes the truth is highly entropic -- which is an easier target to hit, and in which case more models do well on predicting the true distribution of events.\n",
    "\n",
    "Sometimes the truth is low entropy -- in which case few models do well on predicting the true distribution of events.\n",
    "\n",
    "**In the example in the book (Mars and Earth): the planet you're leaving from is $q$ - the model we use to predict the true distribution $p$, the planet we land on.** We have have to make predictions about a true distribution of events (p) using a model that we construct (q)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUT WE DON'T KNOW P** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=vSjL2Zc-gEQ&list=PLDcUM9US4XdMdZOhJWJJD4mDBMnbTWw_z&index=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.4 From divergence to deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point is to:\n",
    "\n",
    "1. How to measure the distance of a model from our target? **Information theory gives us the distance measure we need, the K-L divergence.**\n",
    "2. How to estimate the divergence? **Having identified the right measure of distance, we now need a way to estimate it in real statistical modeling tasks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied log-transform to sd and added transformed sd_log_ to model.\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as brain_model:\n",
    "    pm.glm.glm('brain ~ mass', data)    \n",
    "    map_estimate = pm.find_MAP(model=brain_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.98964376172647"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2 * sum(\n",
    "    stats.norm.logpdf(\n",
    "        data['brain'], \n",
    "        loc=map_estimate['Intercept'] + map_estimate['mass'] * data['mass'], \n",
    "        scale=np.exp(map_estimate['sd_log_'])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.5 From deviance to out-of-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is whack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate the true data\n",
    "\n",
    "x1 = stats.norm.rvs(0, 1, size=40)\n",
    "x2 = stats.norm.rvs(0, 1, size=40)\n",
    "x3 = stats.norm.rvs(0, 10, size=40)\n",
    "x4 = stats.norm.rvs(0, 10, size=40)\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    np.array([x1, x2, x3, x4, 0.15 * x1 - 0.4 * x2]).T, columns=['x1', 'x2', 'x3', 'x4', 'true_line'])\n",
    "\n",
    "data['observed'] = stats.norm.rvs(loc=data.true_line, scale=0.5, size=40)\n",
    "\n",
    "in_sample = data.head(20)\n",
    "out_sample = data.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, name, map_estimate, trace):\n",
    "        self.name = name\n",
    "        self.map_ = map_estimate\n",
    "        self.trace = trace\n",
    "    \n",
    "    def get_map_var(self, varname):\n",
    "        if varname.endswith('log_'):\n",
    "            return np.exp(self.map_[varname])\n",
    "        \n",
    "        return self.map_[varname]\n",
    "    \n",
    "    \n",
    "    def get_coeffs_for_data(self, data):\n",
    "        valid_cols = [col for col in data.columns if col in self.map_.keys()]\n",
    "        return pd.Series([self.get_map_var(col) for col in valid_cols], index=valid_cols)\n",
    "\n",
    "    \n",
    "    def get_loc(self, data):\n",
    "        valid_cols = [col for col in data.columns if col in self.map_.keys()]\n",
    "        \n",
    "        if len(valid_cols):\n",
    "            coeffs = pd.Series([self.get_map_var(col) for col in valid_cols], index=valid_cols)\n",
    "            return self.get_map_var('Intercept') + data[valid_cols].dot(coeffs)\n",
    "        \n",
    "        else:\n",
    "            return self.get_map_var('Intercept')\n",
    "                \n",
    "    \n",
    "    def get_scale(self):\n",
    "        return self.get_map_var('sd_log_')\n",
    "    \n",
    "    \n",
    "    def get_deviance(self, data, points):\n",
    "        locs = self.get_loc(data)\n",
    "        scale = self.get_scale()\n",
    "        \n",
    "        logLik = stats.norm.logpdf(points, loc=locs, scale=scale)\n",
    "        \n",
    "        return -2 * np.sum(logLik)\n",
    "        \n",
    "        \n",
    "class ModelsHolder(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = collections.defaultdict(Model)\n",
    "    \n",
    "    def add_model(self, model):\n",
    "        self.models[model.name] = model\n",
    "    \n",
    "    def get_model(self, model_name):\n",
    "        return self.models[model_name]\n",
    "    \n",
    "    def get_models(self):\n",
    "        return sorted(self.models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied log-transform to sd and added transformed sd_log_ to model.\n",
      "Assigned NUTS to Intercept\n",
      "Assigned NUTS to sd_log_\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 1.1 secApplied log-transform to sd and added transformed sd_log_ to model.\n",
      "Assigned NUTS to Intercept\n",
      "Assigned NUTS to x1\n",
      "Assigned NUTS to sd_log_\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 1.3 secApplied log-transform to sd and added transformed sd_log_ to model.\n",
      "Assigned NUTS to Intercept\n",
      "Assigned NUTS to x1\n",
      "Assigned NUTS to x2\n",
      "Assigned NUTS to sd_log_\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 1.0 secApplied log-transform to sd and added transformed sd_log_ to model.\n",
      "Assigned NUTS to Intercept\n",
      "Assigned NUTS to x1\n",
      "Assigned NUTS to x2\n",
      "Assigned NUTS to x3\n",
      "Assigned NUTS to sd_log_\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 1.2 secApplied log-transform to sd and added transformed sd_log_ to model.\n",
      "Assigned NUTS to Intercept\n",
      "Assigned NUTS to x1\n",
      "Assigned NUTS to x2\n",
      "Assigned NUTS to x3\n",
      "Assigned NUTS to x4\n",
      "Assigned NUTS to sd_log_\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 1.4 sec"
     ]
    }
   ],
   "source": [
    "holder = ModelsHolder()\n",
    "\n",
    "with pm.Model() as m1:\n",
    "    pm.glm.glm('observed ~ 1', in_sample, family=pm.glm.families.Normal()) \n",
    "    \n",
    "    map_estimate = pm.find_MAP()\n",
    "    trace = pm.sample(1000, start=map_estimate)\n",
    "    \n",
    "    holder.add_model(Model('m1', map_estimate, trace))\n",
    "    \n",
    "with pm.Model() as m2:\n",
    "    pm.glm.glm('observed ~ 1 + x1', in_sample)\n",
    "    \n",
    "    map_estimate = pm.find_MAP()\n",
    "    trace = pm.sample(1000, start=map_estimate)\n",
    "    \n",
    "    holder.add_model(Model('m2', map_estimate, trace))\n",
    "   \n",
    "with pm.Model() as m3:\n",
    "    pm.glm.glm('observed ~ 1 + x1 + x2', in_sample)    \n",
    "    \n",
    "    map_estimate = pm.find_MAP()\n",
    "    trace = pm.sample(1000, start=map_estimate)\n",
    "    \n",
    "    holder.add_model(Model('m3', map_estimate, trace))\n",
    "    \n",
    "with pm.Model() as m4:\n",
    "    pm.glm.glm('observed ~ 1 + x1 + x2 + x3', in_sample)    \n",
    "    \n",
    "    map_estimate = pm.find_MAP()\n",
    "    trace = pm.sample(1000, start=map_estimate)\n",
    "    \n",
    "    holder.add_model(Model('m4', map_estimate, trace))\n",
    "    \n",
    "with pm.Model() as m5:\n",
    "    pm.glm.glm('observed ~ 1 + x1 + x2 + x3 + x4', in_sample)    \n",
    "    \n",
    "    map_estimate = pm.find_MAP()\n",
    "    trace = pm.sample(1000, start=map_estimate)\n",
    "    \n",
    "    holder.add_model(Model('m5', map_estimate, trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAEACAYAAACarYNrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b3814fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax_left, ax_right) = plt.subplots(1,2, figsize=(10, 4))\n",
    "\n",
    "for index, model in enumerate(holder.get_models()):\n",
    "    \n",
    "    in_sample_dev = holder.get_model(model).get_deviance(in_sample, in_sample['observed'])\n",
    "    out_sample_dev = holder.get_model(model).get_deviance(out_sample, out_sample['observed'])\n",
    "    \n",
    "    ax_left.scatter(index + 1, in_sample_dev, c='b', label='in_sample')\n",
    "    ax_left.scatter(index + 1, out_sample_dev, c='r', label='out_sample')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviance is like RMSE, sort of. As your model (as defined by MAP) gets closer to the data, the log-likelihood of data gets smaller (because the actual probabilities get larger, therefore the log-probabilities coming from the likelihood get smaller). As the data has less likelihood, given your fit model, the likelihood of the data gets smaller, and the log-likelihood gets larger.\n",
    "\n",
    "This explain why the in-sample deviance gets smaller as you add more parameters -- the model is overfit, it fits well to the data used to train it, but has large out-of-sample deviance, because it doesn't generalize. \n",
    "\n",
    "I don't run the simulation 1e4 times because pymc is slow to fit the models and find map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularizing priors are more conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Information criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 information criteria we have to go through, so best get a move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEPCAYAAABLIROyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nACIiCaUCICKSUCoAIiIJ9f8BTXqP4abOqyYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b45749e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6, 4))\n",
    "\n",
    "for index, model in enumerate(holder.get_models()):\n",
    "    \n",
    "    in_sample_dev = holder.get_model(model).get_deviance(in_sample, in_sample['observed'])\n",
    "    out_sample_dev = holder.get_model(model).get_deviance(out_sample, out_sample['observed'])\n",
    "    \n",
    "    min_dev = min([in_sample_dev, out_sample_dev])\n",
    "    dev_diff = np.abs(in_sample_dev - out_sample_dev)\n",
    "    \n",
    "    ax.scatter(index + 1, in_sample_dev, c='b', label='in_sample')\n",
    "    ax.scatter(index + 1, out_sample_dev, c='r', label='out_sample')\n",
    "    \n",
    "    ax.vlines(index + 1, ymin=in_sample_dev, ymax=out_sample_dev)\n",
    "    \n",
    "    ax.annotate('%.02f' % dev_diff, xy=(index + 1.15, min_dev + dev_diff / 2))\n",
    "    \n",
    "    ax.scatter(index + 1, in_sample_dev + 2 * (index + 1), c='g', label='AIC')\n",
    "    \n",
    "ax.set_xlabel('Number of parameters');\n",
    "ax.set_ylabel('Divergence');\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AIC** (Akaike Information Criterion) provides a surprisingly simple estimate of the average out-of-sample deviance:\n",
    "\n",
    "$$ AIC = D_{train} + 2 * p $$\n",
    "\n",
    "where p is the number of free parameters to be estimated in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIC provides an approximation of predictive accuracy, as measured by out of sample deviance. All information criteria aim at this same target, but are derived under more and less general assumptions. AIC is an approximation that is reliable only when:\n",
    "\n",
    "1. The priors are flat or overwhelmed by the likelihood\n",
    "2. The posterior distribution is approximately multivariate Gaussian\n",
    "3. The sample size N is much greater that the number of parameters k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DIC\n",
    "\n",
    "The **Deviance Information Criterion** accommodates informative priors, but still assumes that the posterior is multivariate Gaussian and that $N>>k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let D be the posterior distribution of deviance -- since the parameters have posterior distributions, and the deviance is computed using the parameters, deviance can have a posterior distribution as well. So if we draw 10.000 samples from the posterior, we can compute 10.000 deviance values. Let $\\bar{D}$ indicate **the average of D**. Also define $\\hat{D}$ as the deviance calculated at the posterior mean --  this means we compute the average of each parameter in the posterior distribution, **the expected value of the parameter**. We can then plug those averages into the deviance formula to get $\\hat{D}$. \n",
    "\n",
    "Once we have $\\bar{D}$ and $\\hat{D}$, DIC is calculated as:\n",
    "\n",
    "$$DIC = \\bar{D} + (\\bar{D} - \\hat{D}) = \\bar{D} + p_{D}$$\n",
    "\n",
    "where the difference $$\\bar{D} - \\hat{D} = p_D$$ is analogous to the number of parameters used in computing AIC. It is an \"effective\" number of parameters that measures how flexible the model is in fitting the training sample. More flexible models entail greater risk of overfitting. So this $p_D$ term is sometimes called a penalty term. It is just the expected distance between the deviance in-sample and the deviance out-of-sample. In the case of flat priors, DIC reduces to AIC, because the expected distance is just the number of parameters. But more generally, $p_D$ will be some fraction of the number of parameters, because regularizing priors constrain a model's flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WAIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better than DIC is the **Widely (Watanabe) Applicable Information Criterion** (WAIC). WAIC has a more complicated definition, but it is also calculated by taking averages of log-likelihood over the posterior distribution. And it is also just an estimate of out-of-sample deviance. But it does not require a multivariate Gaussian posterior, and it is often more accurate than DIC. \n",
    "\n",
    "The distinguishing feature of WAIC is that is is _pointwise_. This means that uncertainty in prediction is considered case-by-case, or point-by-point, in the data (think leave-one-out cross-validation). This is useful, cause some observations are much harder to predict, than others and may also have different uncertainty. It assesses the flexibility of a model with respect to fitting each observation, and then sums up across all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $Pr(y_{i})$ as the average likelihood of observation i in the training sample. This means we compute the likelihood of $y_i$ for each set of parameters sampled from the posterior distribution. Then we average the likelihoods for each observation $i$ and finally sum over all observations. This produces the first part of $WAIC$, the log-pointwise-predictive-density:\n",
    "\n",
    "$$ lppd = \\sum_{i} log Pr(y_i) $$\n",
    "\n",
    "_ The log-pointwise-predictive-density is the total observations of the logarithm of the average likelihood of each observation in the training sample._\n",
    "\n",
    "The lppd is just a pointwise analog of deviance, averaged over the posterior distribution. If you multiplied it by -2, it'd be similar to the deviance, in fact. \n",
    "\n",
    "\n",
    "The second piece of WAIC is the effective number of parameters $p_{WAIC}$. Define $V(y_i)$ as the variance in log-likelihood for observation $i$ in the training sample. This means we compute the log-likelihood of $y_i$ for each sample from the posterior distribution. Then we take the variance of those values. This is $V(y_i)$. Now $p_{WAIC}$ is defined as:\n",
    "\n",
    "$$ p_{WAIC} = \\sum_{i=1}^{N} V(y_i)$$\n",
    "\n",
    "Now WAIC is defined as:\n",
    "\n",
    "$$ WAIC = -2 * (lppd - p_{WAIC}) $$\n",
    "\n",
    "And this value is yet another estimate of out-of-sample deviance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the validity of all information criteria depends upon the predictive task you have in mind. And not all prediction tasks can take the form that we've been assuming for the train-test simulations we've been assuming for the train-test simulations in this chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Using information criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
